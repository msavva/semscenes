---
title: Projects
layout: page
tags: ['page']
pageOrder: 3
---

We have worked on different projects that aims for a <em>semantic</em> understanding of 3D environments.  What does it mean to understand the environment?  Key is to look at and examine environments as composition of objects, and interactions with people.    

These are some of the question we seek to answer: 
- how people interact with scenes
- what names people give to objects in scenes (is the name of objects dependent on the appearance/functionality/context?)
- what are good representations for scenes?  what are important salient features of the environments are most relevant to people?
- how do environments evolve and change over time?

## Scene understanding and representation
Underlying any attempt at understanding of environments, we must consider how environments are to be represented.  Structured representations that allows for manipulation and changes.  We ask whether common knowledge about environments can be learned from data.  What representation do we need that will allow us to generate and synthesis new environments, to allow users to easily manipulate and interact with an virtual environment? 

## Human centric understanding of environments
One important key to understanding environment is to examine how people interact with their surroundings and how the environment is structured and designed to support human activities.  This is especially true of indoor environments, where architects, builders, and designer has specifically created spaces for us to live and work in.  We can view environments as <em>spaces for action</em>.  
- SceneGrok ([webpage](http://graphics.stanford.edu/projects/scenegrok/))
  Where can a person do X?
- Activity based scene synthesis ([webpage](http://graphics.stanford.edu/projects/actsynth/))
  Given that indoor 
- PiGraphs ([webpage](http://graphics.stanford.edu/projects/pigraphs/))

## Scene synthesis
We believe that true understanding goes beyond passive recognition.  That it is important to be able to create new environments.  To that end, we have a series of projects that aims to syntheis new scenes.
- Example based scene synthesis ([webpage](http://graphics.stanford.edu/projects/scenesynth/))   
- Activity based scene synthesis ([webpage](http://graphics.stanford.edu/projects/actsynth/))
- Text to 3D scene generation ([webpage](https://nlp.stanford.edu/data/text2scene.shtml), [demo](https://dovahkiin.stanford.edu/fuzzybox/text2scene.html))
- Scene editing ([demo](https://dovahkiin.stanford.edu/fuzzybox/scene-suggest.html))

## Learning from synthetic data and simulation
Whether we can utilize synthesic scenes to generate training data for learning in a virtual world.
- SSCNet ([webpage](http://vision.princeton.edu/projects/2016/SSCNet/))
- Characterizing Structural Relationships in Scenes using Graph Kernels ([webpage](http://msavva.github.io/files/graphkernel.pdf))
- Physically based rendering ([webpage](http://robots.princeton.edu/projects/2016/PBRS/)])
- Learning where to look ([pdf](https://arxiv.org/abs/1704.02393))




