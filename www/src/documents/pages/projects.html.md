---
title: Projects
layout: page
tags: ['page']
pageOrder: 3
---

We have worked on different projects that aims for a semantic understanding of 3D environments.  We want to understand how people interact with scenes, what names people give to objects to scenes, and how scenes composed and changed overed time.

## Human centric understanding of environments
- PiGraphs (http://graphics.stanford.edu/projects/pigraphs/)
- SceneGrok (http://graphics.stanford.edu/projects/scenegrok/)
- Activity based Scene Synth (http://graphics.stanford.edu/projects/actsynth/)

## Scene synthesis
- Example based scene synth (http://graphics.stanford.edu/projects/scenesynth/)
- Activity based scene synth (http://graphics.stanford.edu/projects/actsynth/)
- Text to 3D scene generation (webpage: https://nlp.stanford.edu/data/text2scene.shtml, demo: https://dovahkiin.stanford.edu/fuzzybox/text2scene.html)
- Scene editing (https://dovahkiin.stanford.edu/fuzzybox/scene-suggest.html)

## Scene Understanding and representation: Learning from synthetic scenes and simulation
- SSCNet (http://vision.princeton.edu/projects/2016/SSCNet/)
- Characterizing Structural Relationships in Scenes using Graph Kernels (http://msavva.github.io/files/graphkernel.pdf)
- Physically based rendering http://robots.princeton.edu/projects/2016/PBRS/
- Learning where to look https://arxiv.org/abs/1704.02393




